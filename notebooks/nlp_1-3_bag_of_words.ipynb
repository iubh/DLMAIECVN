{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_1_3_bag_of_words.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8kWlZt0q7F5"
      },
      "source": [
        "# Bag-of-Words\n",
        "Bag-of-words is a simple approach to convert textual information to numbers. As the name says, we can represent text in the form of an unique set of words (“bag”), i.e. a vector containing word counts of a document [[1]](#scrollTo=MrWOUbg6PsxB).\n",
        "\n",
        "\n",
        "This notebook shows an example of the implementation of bag-of-words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va6Al4QxN_3c"
      },
      "source": [
        "### Create word token\n",
        "\n",
        "We create two sentences ``s1`` and ``s2``. Then, we concatenate both sentences to a single list ``doc``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hnkiu6mG6qsz",
        "outputId": "0c596539-b90a-4df4-bd61-e438d3332ee7"
      },
      "source": [
        "# Create input sentences\n",
        "s1=\"Federer is one of the greatest tennis players of all time.\"\n",
        "s2=\"Federer has won twenty grand slam titles to date.\"\n",
        "\n",
        "# Remove '.' and tokenize both sentences. Then concatenate both sentences to one list.\n",
        "doc = s1.replace('.','').split() + s2.replace('.','').split()\n",
        "\n",
        "# Create an empty list \"unique_tokens\"\n",
        "unique_tokens = []\n",
        "\n",
        "# Iterate over the document \"doc\" and append unique elements t into the list \"unique_tokens\"\n",
        "for t in doc:\n",
        "  if not t in unique_tokens:\n",
        "    unique_tokens.append(t)\n",
        "\n",
        "# Print unique tokens\n",
        "print(unique_tokens)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Federer', 'is', 'one', 'of', 'the', 'greatest', 'tennis', 'players', 'all', 'time', 'has', 'won', 'twenty', 'grand', 'slam', 'titles', 'to', 'date']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzxwvaK7OPFs"
      },
      "source": [
        "### Count word token frequency\n",
        "We count the frequency of each unique word token in the sentences ``s1`` and ``s2``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGF-n6l5FcMS",
        "outputId": "3ee3b4e0-02a1-4e5e-dd5b-0845fa622abb"
      },
      "source": [
        "# Create a list \"vec_1\"\n",
        "vec_1 = []\n",
        "\n",
        "# Remove \".\" from the sentence \"s1\" and tokenize it\n",
        "token_1 = s1.replace('.','').split()\n",
        "\n",
        "# Create a loop to find word token frequency\n",
        "for t in unique_tokens:\n",
        "  ## Iterate over \"unique_tokens\" and assign frequency data to the variable \"count\"\n",
        "  count = token_1.count(t)\n",
        "\n",
        "  # Print frequency data \"count\" of each unique token\n",
        "  print(f'{t}: {count}')\n",
        "\n",
        "  # Append frequency data \"count\" to the list \"vec_1\"\n",
        "  vec_1.append(count)\n",
        "\n",
        "# Print \"vec_1\" and the sentence \"s1\"\n",
        "print(f'\\nVector ouput:\\n\\n{vec_1}\\n{s1}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Federer: 1\n",
            "is: 1\n",
            "one: 1\n",
            "of: 2\n",
            "the: 1\n",
            "greatest: 1\n",
            "tennis: 1\n",
            "players: 1\n",
            "all: 1\n",
            "time: 1\n",
            "has: 0\n",
            "won: 0\n",
            "twenty: 0\n",
            "grand: 0\n",
            "slam: 0\n",
            "titles: 0\n",
            "to: 0\n",
            "date: 0\n",
            "\n",
            "Vector ouput:\n",
            "\n",
            "[1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Federer is one of the greatest tennis players of all time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugB8f6YZHlNL",
        "outputId": "90fd8f00-2812-4d82-d37f-c5663b089f66"
      },
      "source": [
        "# Create a list \"vec_2\"\n",
        "vec_2 = []\n",
        "\n",
        "# Remove \".\" from the sentence \"s2\" and tokenize it\n",
        "token_2 = s2.replace('.','').split()\n",
        "\n",
        "# Create a loop to find word token frequency\n",
        "for t in unique_tokens:\n",
        "\n",
        "  ## Iterate over \"unique_tokens\" and assign frequency data to the variable \"count\"\n",
        "  count = token_2.count(t)\n",
        "\n",
        "  # Print frequency data \"count\" of each unique token\n",
        "  print(f'{t}: {count}')\n",
        "\n",
        "  # Append frequency data \"count\" to the list \"vec_2\"\n",
        "  vec_2.append(count)\n",
        "\n",
        "# Print \"vec_2\" and the sentence \"s2\"\n",
        "print(f'\\nVector ouput:\\n\\n{vec_2}\\n{s2}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Federer: 1\n",
            "is: 0\n",
            "one: 0\n",
            "of: 0\n",
            "the: 0\n",
            "greatest: 0\n",
            "tennis: 0\n",
            "players: 0\n",
            "all: 0\n",
            "time: 0\n",
            "has: 1\n",
            "won: 1\n",
            "twenty: 1\n",
            "grand: 1\n",
            "slam: 1\n",
            "titles: 1\n",
            "to: 1\n",
            "date: 1\n",
            "\n",
            "Vector ouput:\n",
            "\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Federer has won twenty grand slam titles to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfo10R9HO7fg"
      },
      "source": [
        "### Print vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwQ7y9kjHwbw",
        "outputId": "dd3ddc07-a13a-4bb4-c27b-666da21e6129"
      },
      "source": [
        "# Print both vectors\n",
        "print(f'{vec_1}')\n",
        "print(f'{vec_2}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "- [1] Course Book \"NLP and Computer Vision\" (DLMAINLPCV01)\n"
      ],
      "metadata": {
        "id": "MrWOUbg6PsxB"
      }
    },
    {
      "source": [
        "Copyright © 2022 IU International University of Applied Sciences"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "UWRRuAflXcnU"
      }
    }
  ]
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "name": "nlp_2-1_text_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LGamZ8S_zyb"
      },
      "source": [
        "# **Text Summarization**\n",
        "\n",
        "Text summarization in NLP describes methods to automatically generate text summaries containing the most relevant information from source texts. With text summarization, we use extractive and abstractive techniques. In extractive techniques, algorithms extract the most important word sequences of the document to produce a summary of the given text. Abstractive techniques generate summaries by generating a new text and paraphrase the content of the original document, pretty much like humans do when they write an abstract [[1]](#scrollTo=8Pzkt1Z_M6OH).\n",
        "\n",
        "This notebook shows an example of unsupervised extractive text summarization with TextRank."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised extractive text summarization with TextRank\n",
        "\n",
        "TextRank is a common unsupervised extractive summarization technique. It compares every sentence in the text with every other sentence by calculating a similarity score, for example, the cosine similarity for each sentence pair. The closer the score is to 1, the more similar the sentence is to the other sentence representing the other sentences in a good way. These scores are summed up for each sentence to get a rank. The higher the rank, the more important the sentence is in the text. Finally, the sentences can be sorted by rank and a summary can be built from a defined number of highest ranked sentences [[1]](#scrollTo=8Pzkt1Z_M6OH).\n",
        "\n",
        "Unsupervised text summarization can be performed with the ``spaCy`` library and the TextRank algorithm by using the ``pytextrank`` library. For more details about the ``spaCy`` and ``pytextrank`` libraries, please refer to [[2]](https://spacy.io/) and [[3]](https://derwen.ai/docs/ptr/).\n",
        "\n",
        "For text summarization with ``spaCy`` and ``pytextrank``, we will apply the following steps:\n",
        "* Install and import libraries\n",
        "* Download and install the language model\n",
        "* Create a ``spaCy`` pipeline and load the language model to it\n",
        "* Add ``pytextrank`` to the ``spaCy`` pipeline\n",
        "* Create a ``spaCy`` document with a sample text\n",
        "* Use the ``textrank.summary()`` method to create a text summary"
      ],
      "metadata": {
        "id": "3HEtUqo6Z2lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import libraries"
      ],
      "metadata": {
        "id": "L-mzk2N8Ay6B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xavxSpS-f3CI"
      },
      "source": [
        "#### Install ``pytextrank`` library\n",
        "\n",
        "``pytextrank`` is an implementation of TextRank for the use in ``spaCy`` pipelines. It provides fast, effective phrase extraction from texts, along with an extractive summarization [[4]](https://spacy.io/universe/project/spacy-pytextrank).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQIo1p4uAC8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96f5d98e-a2c1-43d6-dc8e-e4bb50acc2cc"
      },
      "source": [
        "# Install the pytextrank library \n",
        "!pip install pytextrank==3.0.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytextrank==3.0.1\n",
            "  Downloading pytextrank-3.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting icecream>=2.1\n",
            "  Downloading icecream-2.1.2-py2.py3-none-any.whl (8.3 kB)\n",
            "Requirement already satisfied: spacy>=3.0 in /usr/local/lib/python3.7/dist-packages (from pytextrank==3.0.1) (3.3.1)\n",
            "Collecting graphviz>=0.13\n",
            "  Downloading graphviz-0.20-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from pytextrank==3.0.1) (2.6.3)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from icecream>=2.1->pytextrank==3.0.1) (2.6.1)\n",
            "Collecting asttokens>=2.0.1\n",
            "  Downloading asttokens-2.0.5-py2.py3-none-any.whl (20 kB)\n",
            "Collecting colorama>=0.3.9\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Collecting executing>=0.3.1\n",
            "  Downloading executing-0.8.3-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank==3.0.1) (1.15.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (8.0.17)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (3.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.0.7)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.6.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.7.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.9.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (4.64.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (1.0.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (2.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank==3.0.1) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0->pytextrank==3.0.1) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.0->pytextrank==3.0.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0->pytextrank==3.0.1) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank==3.0.1) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0->pytextrank==3.0.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0->pytextrank==3.0.1) (2.0.1)\n",
            "Installing collected packages: executing, colorama, asttokens, icecream, graphviz, pytextrank\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed asttokens-2.0.5 colorama-0.4.5 executing-0.8.3 graphviz-0.20 icecream-2.1.2 pytextrank-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HsF9N9igEwQ"
      },
      "source": [
        "#### Import ``spaCy`` and ``pytextrank`` libraries\n",
        "\n",
        "We import the ``spaCy`` and ``pytextrank`` libraries.\n",
        "\n",
        "``spaCy`` is a free, open-source library for advanced Natural Language Processing (NLP) in Python. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning [[5]](https://spacy.io/usage/spacy-101). It supports the implementation of tasks for sentiment analysis, chatbots, text summarization, intent and entity extraction, and others [[1]](#scrollTo=8Pzkt1Z_M6OH). More information about ``spaCy`` please refer to  [[2]](https://spacy.io/).\n",
        "\n",
        "We have installed the ``pytextrank`` library in the previous section. Now we will import it. For more details about ``pytextrank``, please refer to [[3]](https://derwen.ai/docs/ptr/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa3QK56i_zyx"
      },
      "source": [
        "# Import spaCy and pytextrank libraries\n",
        "import spacy\n",
        "import pytextrank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and install language model\n",
        "We load the ``en_core_web_sm`` English language model by using the ``spaCy`` library.\n",
        "For more details about ``en_core_web_sm``, please refer to [[6]](https://spacy.io/models)."
      ],
      "metadata": {
        "id": "ibonn_5oG5BP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhdAZJnug3T8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634d3d32-a130-416f-f33a-dac871304dd6"
      },
      "source": [
        "# Download \"en_core_web_sm\" English language model\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create ``spaCy`` pipeline and load language model\n",
        "We use the ``spacy.load()`` function to load our language model ``en_core_web_sm`` to the ``spaCy`` pipeline ``sp``.\n"
      ],
      "metadata": {
        "id": "WXS91gcsCCKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a spaCy pipeline \"sp\" and load the language model\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "fL34gYV6CbWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add ``pytextrank`` to ``spaCy`` pipeline\n",
        "\n",
        "We use the ``add_pipe()`` method to add ``pytextrank`` to the ``spaCy`` pipeline ``sp``."
      ],
      "metadata": {
        "id": "fz8gz5rWIMDw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upP_NK5B_zy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01870107-38c0-4031-972d-eb6ae0b7ea6a"
      },
      "source": [
        "# Add pytextrank to the spaCy pipeline\n",
        "sp.add_pipe('textrank', last=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pytextrank.base.BaseTextRank at 0x7fc5aeb8dd50>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our ``spaCy`` pipeline is ready for text summarization. In the following step, we will create a ``spaCy`` document for text summarization."
      ],
      "metadata": {
        "id": "XKLEemdLBE_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create ``spaCy`` document with sample text\n",
        "\n",
        "In this step, we add a sample text to the ``spaCy`` pipeline and create a ``Doc`` object as ``doc``.\n",
        "\n",
        "When we create a ``Doc`` object by using the ``spaCy`` library, it automatically performs tokenization, named entity recognition (NER) and part-of-speech (POS) tagging processes for an input text. The following figure demonstrates the processing pipeline of a given text to create a ``Doc`` object [[7]](https://spacy.io/usage/processing-pipelines).\n",
        "\n",
        "![spaCy](https://spacy.io/pipeline-fde48da9b43661abcdf62ab70a546d71.svg)"
      ],
      "metadata": {
        "id": "vABivFctEqvw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKOHaC9Me80Z"
      },
      "source": [
        "# Define a sample text\n",
        "text=\"\"\"Alan Mathison Turing, a British mathematician and computer scientist,\\\n",
        " was one of the early pioneers of artificial intelligence. Turing (1950) describes \\\n",
        " the foundation of what was later called the Turing test. The experimental setup of \\\n",
        " the Turing test is as follows. A human interrogator uses a chat program to talk to \\\n",
        " two conversation partners: a chatbot and another human being. Both of them try to \\\n",
        " convince the interrogator that they are the human. If the interrogator is not able to \\\n",
        " identify the human through intense questioning, the machine is considered to have passed \\\n",
        " the Turing test. According to Turing, passing the test can lead to the conclusion that \\\n",
        " the machine’s intellectual power is on a level comparable to the human brain. While the \\\n",
        " Turing test has often been criticized because of its focus on functionality, the question \\\n",
        " of whether the machine is conscious about its answers remains open. Several attempts have \\\n",
        " been made to pass the Turing test, but it still remains an unresolved challenge.\"\"\"\n",
        "\n",
        "# Create a spaCy Doc object \"doc\" with the sample text\n",
        "doc = sp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create text summary\n",
        "\n",
        "We use the ``textrank.summary()`` method of ``pytextrank`` to run an extractive summarization. For that, we set the following parameters:\n",
        "\n",
        "* ``limit_phrases``: It defines the maximum number of top-ranked phrases.  ``pytextrank`` calculates a cosine similarity score for each phrase in the given text and sorts in descending order. In this example, we set ``limit_phrases=3``. That means we take the first 3 top-ranked phrases of each sentence. The sum of these 3 phrases is used to calculate the importance of each sentence. Since ``limit_phrases`` is a hyperparameter, you can set different limits.\n",
        "\n",
        "* ``limit_sentences``: It defines the total number of sentences to return. In this example, we set ``limit_sentences=3``. That means our summary will contain 3 sentences.\n",
        "\n",
        "* ``preserve_order``: It preserves the order of sentences as they originally occurred in the given text. In this example, we set ``preserve_order=True``.\n",
        "\n",
        "The ``textrank.summary()`` method automatically performs the following steps:\n",
        "* Calculate a similarity score for each phrase in the sample text\n",
        "* Rank each sentence by the total similarity score calculated from the top P phrases (P=``limit_phrases``)\n",
        "* Return the S sentences with the highest total similarity score (S=``limit_sentences``) as the text summary\n",
        "\n",
        "For more details about the ``textrank.summary()`` method, please refer to [[8]](https://derwen.ai/docs/ptr/ref/).\n",
        "\n"
      ],
      "metadata": {
        "id": "yxRGgCDhBdEO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tZyjO0krJbk",
        "outputId": "129130fc-0d10-4b72-f67f-2977eb73134b"
      },
      "source": [
        "# Perform text summarization\n",
        "summary = list(doc._.textrank.summary(limit_phrases=3, limit_sentences=3, preserve_order=True))\n",
        "for sent in summary:\n",
        "  print(sent,\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alan Mathison Turing, a British mathematician and computer scientist, was one of the early pioneers of artificial intelligence. \n",
            "\n",
            "Turing (1950) describes  the foundation of what was later called the Turing test. \n",
            "\n",
            "According to Turing, passing the test can lead to the conclusion that  the machine’s intellectual power is on a level comparable to the human brain. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "- [1] Course Book \"NLP and Computer Vision\" (DLMAINLPCV01)\n",
        "- [2] https://spacy.io\n",
        "- [3] https://derwen.ai/docs/ptr\n",
        "- [4] https://spacy.io/universe/project/spacy-pytextrank\n",
        "- [5] https://spacy.io/usage/spacy-101\n",
        "- [6] https://spacy.io/models\n",
        "- [7] https://spacy.io/usage/processing-pipelines\n",
        "- [8] https://derwen.ai/docs/ptr/ref"
      ],
      "metadata": {
        "id": "8Pzkt1Z_M6OH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2oMgGVZAivX"
      },
      "source": [
        "Copyright © 2022 IU International University of Applied Sciences"
      ]
    }
  ]
}

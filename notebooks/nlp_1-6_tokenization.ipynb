{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "name": "nlp_1-6_tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_X42HQ1pF84"
      },
      "source": [
        "# **Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of splitting up textual input data, e.g., a sentence or a paragraph into meaningful units [[1]](#scrollTo=op-j6UywUt5i).\n",
        "\n",
        "This notebook shows examples of word and sentence tokenization with ``spaCy``."
      ],
      "metadata": {
        "id": "p07VlvadPlrX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CPahqp-pF9i"
      },
      "source": [
        "## **Word tokenization**\n",
        "\n",
        "Word tokenization frequently uses word pieces such as word stems, prefixes,\n",
        "and suffixes. \n",
        "\n",
        "For word tokenization, we will apply the following steps:\n",
        "1. Import the ``spaCy`` library\n",
        "2. Load the language model (English)\n",
        "3. Create a ``Doc`` object and perform word tokenization\n",
        "4. Print the word tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import ``spaCy`` library\n",
        "``spaCy`` is a free, open-source library for advanced natural language processing (NLP) in Python. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning [[2]](https://spacy.io/usage/spacy-101). For example, it supports the implementation of tasks for sentiment analysis, chatbots, text summarization, intent and entity extraction, and others [[1]](#scrollTo=op-j6UywUt5i). For more information about ``spaCy``, please refer to  [[3]](https://spacy.io/)."
      ],
      "metadata": {
        "id": "7mLLi589N-M2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYFaXafBN12i"
      },
      "source": [
        "# Import the spaCy library\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load language model\n",
        "We will load the ``en_core_web_sm`` English language model by using the ``spaCy`` library.\n",
        "For more details about ``en_core_web_sm``, please refer to [[4]](https://spacy.io/models)."
      ],
      "metadata": {
        "id": "dPvXKO7tOJQR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rVtW6JEpF9f"
      },
      "source": [
        "# Load \"en_core_web_sm\" English language model\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a ``Doc`` object and perform word tokenization\n",
        "\n",
        "When we create a ``Doc`` object by using the ``spaCy`` library, it automatically produces tokens for the input text. The following figure demonstrates the processing pipeline of a given text to create a ``Doc`` object [[5]](https://spacy.io/usage/processing-pipelines):\n",
        "\n",
        "![spaCy](https://spacy.io/pipeline-fde48da9b43661abcdf62ab70a546d71.svg)"
      ],
      "metadata": {
        "id": "hXK00FUnQb97"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n_iU6SVpF9k"
      },
      "source": [
        "# Create a Doc object \"doc\"\n",
        "doc = sp(u'I am non-vegetarian, please send me the menu to abs-xyz@gmail.com. \"They are going to the U.K. and then to the U.S.A\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print word tokens"
      ],
      "metadata": {
        "id": "yrKPnbDRQ3gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print each word token in the object \"doc\":\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWMAW7r2qb4_",
        "outputId": "c0f574fa-5587-4dbc-cb55-0f63c08ea3ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "am\n",
            "non\n",
            "-\n",
            "vegetarian\n",
            ",\n",
            "please\n",
            "send\n",
            "me\n",
            "the\n",
            "menu\n",
            "to\n",
            "abs-xyz@gmail.com\n",
            ".\n",
            "\"\n",
            "They\n",
            "are\n",
            "going\n",
            "to\n",
            "the\n",
            "U.K.\n",
            "and\n",
            "then\n",
            "to\n",
            "the\n",
            "U.S.A\n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV0iRL_ZpF9o"
      },
      "source": [
        "## Sentence tokenization\n",
        "Sentence tokenization is used to split up a given text into individual sentences [[1]](#scrollTo=op-j6UywUt5i). \n",
        "\n",
        "In the section [\"Word tokenization\"](#scrollTo=3CPahqp-pF9i), we have already imported the ``spaCy`` library, loaded the language model and created a ``Doc`` object. We will use the same ``Doc`` object in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print sentence tokens\n",
        "\n",
        "As explained [before](#scrollTo=hXK00FUnQb97), ``spaCy`` automatically performs tokenization during creation of the ``Doc`` object . \n",
        "\n",
        "To print the sentence tokens, we will simply use the ``doc.sents`` attribute of the \"Sentencizer\" class in ``spaCy``. For more details, please refer to [[6]](https://spacy.io/api/sentencizer)."
      ],
      "metadata": {
        "id": "aC6BLU8ZSQlo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYFYuprlpF9q",
        "outputId": "baa1a120-491b-4834-ac82-0ebfb734e3d6"
      },
      "source": [
        "# Use the \"doc.sents\" attribute for sentence tokenization.\n",
        "## It iterates over sentences in the document.\n",
        "## Then it defines the first word token of each sentence.\n",
        "## To decide whether a token starts a sentence, spaCy assigns a boolean value to each token.\n",
        "## This will be either True or False for all tokens.\n",
        "for sentence in doc.sents:\n",
        "    print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am non-vegetarian, please send me the menu to abs-xyz@gmail.com.\n",
            "\"They are going to the U.K. and then to the U.S.A\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "- [1] Course Book \"NLP and Computer Vision\" (DLMAINLPCV01)\n",
        "- [2] https://spacy.io/usage/spacy-101\n",
        "- [3] https://spacy.io\n",
        "- [4] https://spacy.io/models\n",
        "- [5] https://spacy.io/usage/processing-pipelines\n",
        "- [6] https://spacy.io/api/sentencizer"
      ],
      "metadata": {
        "id": "op-j6UywUt5i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kDAQDmXpF91"
      },
      "source": [
        "Copyright Â© 2022 IU International University of Applied Sciences"
      ]
    }
  ]
}

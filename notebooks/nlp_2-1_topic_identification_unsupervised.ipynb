{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_2-1_topic_identification_unsupervised.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnKiMO3bqBgU"
      },
      "source": [
        "# **Topic Modeling**\n",
        "Topic identification is the challenge of automatically finding topics\n",
        "in a given text. This can be done in supervised and unsupervised ways. For example, an algorithm labels newspaper articles with known topics such\n",
        "as ”sports,” ”politics,” or ”culture.” In this case, we have predefined topics and labeled training data and could train our model in a supervised way. This is called topic classification. If we do not know the topics in advance and want our algorithm to find clusters of similar topics, we deal with topic modeling or topic discovery, in an unsupervised way [[1]](#scrollTo=1eUuDaNxZ_ms).\n",
        "\n",
        "\n",
        "This notebook shows examples of unsupervised topic modeling with Gensim’s LDA model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc_rZCXXyJJe"
      },
      "source": [
        "## **Unsupervised topic modeling with Gensim’s LDA model**\n",
        "\n",
        "Latent Dirichlet Allocation (LDA) is a common technique used for unsupervised topic modeling. This method uses document embeddings, i.e., vector representations of documents. Then the vector’s dimensionality is reduced with techniques such as singular value decomposition (SVD). Unsupervised topic modeling techniques are often used as a preprocessing step for supervised topic identification [[1]](#scrollTo=1eUuDaNxZ_ms)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the ``nltk`` library and download ``wordnet``\n",
        "``nltk``(Natural Language Toolkit) is an open source Python library for natural language processing. For more details about the ``nltk`` library, please refer to [[2]](https://www.nltk.org/api/nltk.html#nltk.wsd.lesk).\n",
        "\n",
        "``wordnet`` is a lexical database of semantic relations between words in more than 200 languages. It links words into semantic relations [[3]](https://en.wikipedia.org/wiki/WordNet)."
      ],
      "metadata": {
        "id": "pDmDa7Fc-QvB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R5-7qiucf3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c39d80-fe0b-4374-93ca-8f630c95e979"
      },
      "source": [
        "# Import the \"nltk\" module\n",
        "import nltk\n",
        "\n",
        "# Download the \"wordnet\" package by using the \"nltk\" module\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Import \"RegexpTokenizer\"\n",
        "## It is used to split a string into substrings using regular expressions\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Import \"WordNetLemmatizer\"\n",
        "## It is used to lemmatize the words using WordNet's built-in morphy function\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "# Download 'omw-1.4' to use Multilingual Wordnet Data from OMW with newer Wordnet versions (December 2021 release)\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Import Python \"pprint\" library to print complex data structures in an easy to read format\n",
        "## For more details about the difference between print() and pprint() functions, please refer to [4]\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import ``gensim``\n",
        "``gensim`` is a Python library for topic modeling. It enables the extraction of topics in an unsupervised way using LDA.\n",
        "For more details about Gensim's LDA model, please refer to [[5]](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html)."
      ],
      "metadata": {
        "id": "myD112OO-i5Q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ieidCq_yXdl"
      },
      "source": [
        "# Import gensim\n",
        "from gensim import corpora, models\n",
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create documents\n",
        "The input of our model is a list of paragraphs, where each paragraph represents a document containing a specific topic."
      ],
      "metadata": {
        "id": "OkHyUtBD-4dv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXdV4e83TTGP"
      },
      "source": [
        "# Create a list \"doc_list\" containing two documents\n",
        "doc_list = [\n",
        "           \"Black holes are dense points in space and they create deep gravity sinks. It is called black hole because beyond a certain region, not even light can escape the powerful tug of a black hole.\",\n",
        "           \"The Italian explorer Christopher Columbus officially set foot in America, and claimed the land for Spain in October 12, 1492. Americans celebrate Columbus Day as a national holiday every year since 1937. This day is celebrated as Columbus Day in the United States, but the name varies on the international spectrum.\"\n",
        "           ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize documents\n",
        "We have created a list containing two documents. Now each document has to be converted into a bag-of-words format. The bag-of-words format consists of tupels which contain a unique ID for the word type and its absolute frequency. To retrieve the bag-of-words format, each document has to be tokenized first."
      ],
      "metadata": {
        "id": "zhKusjIe-9R-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uEDkNc-TXLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58bc4220-78d9-4c6e-9a4e-791dfa66c065"
      },
      "source": [
        "# Define a tokenizer to split each string into substrings\n",
        "## '\\w+' matches one or more alphanumeric characters\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# Convert text to lowercase and tokenize\n",
        "for idx in range(len(doc_list)):\n",
        "    doc_list[idx] = doc_list[idx].lower()  \n",
        "    doc_list[idx] = tokenizer.tokenize(doc_list[idx])\n",
        "\n",
        "# Remove numbers, but not words that contain numbers\n",
        "doc_list = [[token for token in doc if not token.isnumeric()] for doc in doc_list]\n",
        "\n",
        "# Remove tokens that are only one character\n",
        "doc_list = [[token for token in doc if len(token) > 2] for doc in doc_list]\n",
        "\n",
        "# Print tokens\n",
        "pprint(doc_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['black',\n",
            "  'holes',\n",
            "  'are',\n",
            "  'dense',\n",
            "  'points',\n",
            "  'space',\n",
            "  'and',\n",
            "  'they',\n",
            "  'create',\n",
            "  'deep',\n",
            "  'gravity',\n",
            "  'sinks',\n",
            "  'called',\n",
            "  'black',\n",
            "  'hole',\n",
            "  'because',\n",
            "  'beyond',\n",
            "  'certain',\n",
            "  'region',\n",
            "  'not',\n",
            "  'even',\n",
            "  'light',\n",
            "  'can',\n",
            "  'escape',\n",
            "  'the',\n",
            "  'powerful',\n",
            "  'tug',\n",
            "  'black',\n",
            "  'hole'],\n",
            " ['the',\n",
            "  'italian',\n",
            "  'explorer',\n",
            "  'christopher',\n",
            "  'columbus',\n",
            "  'officially',\n",
            "  'set',\n",
            "  'foot',\n",
            "  'america',\n",
            "  'and',\n",
            "  'claimed',\n",
            "  'the',\n",
            "  'land',\n",
            "  'for',\n",
            "  'spain',\n",
            "  'october',\n",
            "  'americans',\n",
            "  'celebrate',\n",
            "  'columbus',\n",
            "  'day',\n",
            "  'national',\n",
            "  'holiday',\n",
            "  'every',\n",
            "  'year',\n",
            "  'since',\n",
            "  'this',\n",
            "  'day',\n",
            "  'celebrated',\n",
            "  'columbus',\n",
            "  'day',\n",
            "  'the',\n",
            "  'united',\n",
            "  'states',\n",
            "  'but',\n",
            "  'the',\n",
            "  'name',\n",
            "  'varies',\n",
            "  'the',\n",
            "  'international',\n",
            "  'spectrum']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dictionary\n",
        "To create unique IDs for each word type which are needed in the bag-of-words format, we use the ``Dictionary()`` method which takes the tokens and creates a mapping between the word types and their IDs. This mapping which contains the unique ID as ``key`` and the word type as ``value`` is called ``dictionary``."
      ],
      "metadata": {
        "id": "LmxZoNof_QsH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUNr0cereINZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a8bdcd-3822-411e-c23f-0e5c022acf72"
      },
      "source": [
        "# Create a dictionary representation of the \"doc_list\"\"\n",
        "dictionary = corpora.Dictionary(doc_list)\n",
        "\n",
        "# Print keys and values of the \"dictionary\"\n",
        "for key, value in dictionary.items():\n",
        "  print(key, \" : \", value)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  :  and\n",
            "1  :  are\n",
            "2  :  because\n",
            "3  :  beyond\n",
            "4  :  black\n",
            "5  :  called\n",
            "6  :  can\n",
            "7  :  certain\n",
            "8  :  create\n",
            "9  :  deep\n",
            "10  :  dense\n",
            "11  :  escape\n",
            "12  :  even\n",
            "13  :  gravity\n",
            "14  :  hole\n",
            "15  :  holes\n",
            "16  :  light\n",
            "17  :  not\n",
            "18  :  points\n",
            "19  :  powerful\n",
            "20  :  region\n",
            "21  :  sinks\n",
            "22  :  space\n",
            "23  :  the\n",
            "24  :  they\n",
            "25  :  tug\n",
            "26  :  america\n",
            "27  :  americans\n",
            "28  :  but\n",
            "29  :  celebrate\n",
            "30  :  celebrated\n",
            "31  :  christopher\n",
            "32  :  claimed\n",
            "33  :  columbus\n",
            "34  :  day\n",
            "35  :  every\n",
            "36  :  explorer\n",
            "37  :  foot\n",
            "38  :  for\n",
            "39  :  holiday\n",
            "40  :  international\n",
            "41  :  italian\n",
            "42  :  land\n",
            "43  :  name\n",
            "44  :  national\n",
            "45  :  october\n",
            "46  :  officially\n",
            "47  :  set\n",
            "48  :  since\n",
            "49  :  spain\n",
            "50  :  spectrum\n",
            "51  :  states\n",
            "52  :  this\n",
            "53  :  united\n",
            "54  :  varies\n",
            "55  :  year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create bag-of-words representation\n",
        "We use the ``doc2bow()`` method to compute the frequency of each word and convert the ``dictionary`` into the bag-of-words format. The paragraphs in the bag-of-words format serve as the ``corpus`` to train our topic model."
      ],
      "metadata": {
        "id": "qvOgNxZR_ZfX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfWdnHOgeqOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5454c59-ce6c-45e3-c53d-39ad276f5ad2"
      },
      "source": [
        "# Create bag-of-words representation of the documents \n",
        "corpus = [dictionary.doc2bow(doc) for doc in doc_list]\n",
        "\n",
        "# Print \"corpus\"\n",
        "pprint(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(0, 1),\n",
            "  (1, 1),\n",
            "  (2, 1),\n",
            "  (3, 1),\n",
            "  (4, 3),\n",
            "  (5, 1),\n",
            "  (6, 1),\n",
            "  (7, 1),\n",
            "  (8, 1),\n",
            "  (9, 1),\n",
            "  (10, 1),\n",
            "  (11, 1),\n",
            "  (12, 1),\n",
            "  (13, 1),\n",
            "  (14, 2),\n",
            "  (15, 1),\n",
            "  (16, 1),\n",
            "  (17, 1),\n",
            "  (18, 1),\n",
            "  (19, 1),\n",
            "  (20, 1),\n",
            "  (21, 1),\n",
            "  (22, 1),\n",
            "  (23, 1),\n",
            "  (24, 1),\n",
            "  (25, 1)],\n",
            " [(0, 1),\n",
            "  (23, 6),\n",
            "  (26, 1),\n",
            "  (27, 1),\n",
            "  (28, 1),\n",
            "  (29, 1),\n",
            "  (30, 1),\n",
            "  (31, 1),\n",
            "  (32, 1),\n",
            "  (33, 3),\n",
            "  (34, 3),\n",
            "  (35, 1),\n",
            "  (36, 1),\n",
            "  (37, 1),\n",
            "  (38, 1),\n",
            "  (39, 1),\n",
            "  (40, 1),\n",
            "  (41, 1),\n",
            "  (42, 1),\n",
            "  (43, 1),\n",
            "  (44, 1),\n",
            "  (45, 1),\n",
            "  (46, 1),\n",
            "  (47, 1),\n",
            "  (48, 1),\n",
            "  (49, 1),\n",
            "  (50, 1),\n",
            "  (51, 1),\n",
            "  (52, 1),\n",
            "  (53, 1),\n",
            "  (54, 1),\n",
            "  (55, 1)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set training parameters and create model\n",
        "After we have generated the ``corpus`` to train our topic model, we set the following training parameters:\n",
        "* ``num_topics``: It presents the number of topics (number of dimensions) and can be freely chosen. In this example, we set ``num_topics = 2``. It means we ask our unsupervised clustering algorithm to group our dataset into 2 topics. [[1]](#scrollTo=1eUuDaNxZ_ms).\n",
        "* ``chunksize``: It controls how many documents are processed at a time in the training algorithm. Increasing the ``chunksize`` speeds up the training process. In this example, we set ``chunksize = 10``, which is more than the number of documents, so we process all data at a single time. ``chunksize`` can influence the quality of the model [[5]](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html).\n",
        "* ``passes``: It controls how often we train the model on the entire corpus. Another word for passes might be “epochs”. In this example, we set ``passes = 10``.\n",
        "* ``iterations``: It defines how often we repeat a particular loop over each document [[5]](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html). In this example, we set ``iterations = 400``.\n",
        "* ``alpha``: It is a hyperparameter that controls the prior probability distribution over topic weights in each document. In this example, we set  ``alpha='auto'``.\n",
        "* ``eta``: It is a hyperparameter that controls the prior probability distribution over word weights in each topic. In this example, we set  ``eta='auto'``.\n"
      ],
      "metadata": {
        "id": "8rwQnNHnAToP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcmRi7AseqIX"
      },
      "source": [
        "# Set training parameters\n",
        "num_topics = 2\n",
        "chunksize = 10\n",
        "passes = 10\n",
        "iterations = 400\n",
        "alpha='auto'\n",
        "eta='auto'\n",
        "\n",
        "# Create a dictionary \"id2word\"\n",
        "temp = dictionary[0]  # This is only to load the dictionary\n",
        "id2word = dictionary.id2token\n",
        "\n",
        "# Create topic model\n",
        "model = gensim.models.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    chunksize=chunksize,\n",
        "    alpha=alpha, \n",
        "    eta=eta,\n",
        "    iterations=iterations,\n",
        "    num_topics=num_topics,\n",
        "    passes=passes,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print the top 10 words\n",
        "In this section, we use the ``show_topics()`` method to list the 10 words with the highest probability to represent each topic. The words are listed in descending order by their topic-specific probabilities  [[6]](https://mimno.infosci.cornell.edu/papers/mimno-semantic-emnlp.pdf). Gensim's LDA model uses the words frequencies of each document to calculate the topic-specific propability. The sum of all topic-specific probabilities is 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "F-WI4lYeBXXw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI1olErdfjXV",
        "outputId": "b4f40f37-90ce-49ed-dca9-ba14f04b0996"
      },
      "source": [
        "# Use \"show_topics()\" method to list topic-specific probability of each token\n",
        "## \"num_words\" is used to define the number of tokens to be listed for each topic\n",
        "## Set \"formattted=True\" to list tokens and their topic-specific probability scores as string\n",
        "## Set \"formatted=False\" to list as tuple\n",
        "top_10_word_tokens = model.show_topics(num_words=10, formatted=False)\n",
        "\n",
        "# Print the top 10 word tokens with their topic-specific probabilities\n",
        "pprint(top_10_word_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  [('the', 0.059438754),\n",
            "   ('black', 0.053388406),\n",
            "   ('hole', 0.037879664),\n",
            "   ('and', 0.032304194),\n",
            "   ('beyond', 0.0217393),\n",
            "   ('even', 0.021736633),\n",
            "   ('sinks', 0.021733694),\n",
            "   ('create', 0.021722166),\n",
            "   ('are', 0.021717642),\n",
            "   ('deep', 0.021714563)]),\n",
            " (1,\n",
            "  [('the', 0.10077568),\n",
            "   ('columbus', 0.04682967),\n",
            "   ('day', 0.046802428),\n",
            "   ('and', 0.028567936),\n",
            "   ('this', 0.019111384),\n",
            "   ('christopher', 0.019109491),\n",
            "   ('holiday', 0.019108094),\n",
            "   ('spain', 0.019106908),\n",
            "   ('year', 0.019106394),\n",
            "   ('land', 0.019102821)])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print a topic representation\n",
        "In this section, we choose 3 words which have the highest topic-specific probabilities and print them as topic representations."
      ],
      "metadata": {
        "id": "ecHc6e9ATcMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top 3 word tokens representing each topic\n",
        "for index, topic in model.show_topics(formatted=False, num_words= 3):\n",
        "    print('Topic: {} \\nTokens: {}'.format(index, ' '.join([w[0] for w in topic])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx7-RhAapkSU",
        "outputId": "5a409051-c219-4d33-d9bc-2cedd5dcc16b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0 \n",
            "Tokens: the black hole\n",
            "Topic: 1 \n",
            "Tokens: the columbus day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "- [1] Course Book \"NLP and Computer Vision\" (DLMAINLPCV01)\n",
        "- [2] https://www.nltk.org/api/nltk.html#nltk.wsd.lesk\n",
        "- [3] https://en.wikipedia.org/wiki/WordNet\n",
        "- [4] https://docs.python.org/3/library/pprint.html\n",
        "- [5] https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html\n",
        "- [6] https://mimno.infosci.cornell.edu/papers/mimno-semantic-emnlp.pdf"
      ],
      "metadata": {
        "id": "1eUuDaNxZ_ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright © 2022 IU International University of Applied Sciences"
      ],
      "metadata": {
        "id": "kF9klNmjaDob"
      }
    }
  ]
}
